{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOZbd2GsiFPs","outputId":"280a5da6-f58d-4c67-db1c-0c21251c5147","executionInfo":{"status":"ok","timestamp":1709179764824,"user_tz":-330,"elapsed":2435,"user":{"displayName":"Ashiq Firoz","userId":"12908176422254364099"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-02-29 04:09:23--  https://raw.githubusercontent.com/GL3MON/inpLSTM/main/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1155392 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   1.10M  --.-KB/s    in 0.04s   \n","\n","2024-02-29 04:09:24 (29.5 MB/s) - ‘input.txt’ saved [1155392/1155392]\n","\n","--2024-02-29 04:09:24--  https://github.com/GL3MON/inpLSTM/raw/main/LSTM_256.h5\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/GL3MON/inpLSTM/main/LSTM_256.h5 [following]\n","--2024-02-29 04:09:25--  https://raw.githubusercontent.com/GL3MON/inpLSTM/main/LSTM_256.h5\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3321760 (3.2M) [application/octet-stream]\n","Saving to: ‘LSTM_256.h5’\n","\n","LSTM_256.h5         100%[===================>]   3.17M  --.-KB/s    in 0.06s   \n","\n","2024-02-29 04:09:25 (57.4 MB/s) - ‘LSTM_256.h5’ saved [3321760/3321760]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/GL3MON/inpLSTM/main/input.txt\n","!wget https://github.com/GL3MON/inpLSTM/raw/main/LSTM_256.h5"]},{"cell_type":"markdown","metadata":{"id":"zuaFJIejjTaL"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KWTqz18RjIds"},"outputs":[],"source":["import sys\n","import numpy as np\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"markdown","source":["GATHERING DATA:\n","We read the input.txt file and convert everything to lowercase and store it in varaible raw_text"],"metadata":{"id":"xhNy7Ld7M9SJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hM6fDjQpjUcD"},"outputs":[],"source":["def gatheringData(filename):\n","  filename = filename\n","  raw_text = open(filename, 'r', encoding='utf-8').read()\n","  raw_text = raw_text.lower()\n","  return raw_text"]},{"cell_type":"markdown","source":["PRE-PROCESSING DATA:\n","We create a two dictionary which has letters mapped to it's respective integer token and vice-versa.\n","We create training data which has letterwise tokenized senetences as input(X) and the next letter after the sentence as the label(Y).\n","Then we do a test and evaluation split."],"metadata":{"id":"BFw-TYE4NQY4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ud88L0Vj1FM"},"outputs":[],"source":["def preProcessingData(raw_text):\n","  chars = sorted(list(set(raw_text)))\n","  char_to_int = dict((c, i) for i, c in enumerate(chars))\n","  int_to_char = dict((i, c) for i, c in enumerate(chars))\n","  n_chars = len(raw_text)\n","  n_vocab = len(chars)\n","  print(\"Total Characters: \", n_chars)\n","  print(\"Total Vocab: \", n_vocab)\n","  seq_length = 100\n","  dataX = []\n","  dataY = []\n","  for i in range(0, n_chars - seq_length, 1):\n","    seq_in = raw_text[i:i + seq_length]\n","    seq_out = raw_text[i + seq_length]\n","    dataX.append([char_to_int[char] for char in seq_in])\n","    dataY.append(char_to_int[seq_out])\n","  n_patterns = len(dataX)\n","  print(\"Total Patterns: \", n_patterns)\n","  X = np.reshape(dataX, (n_patterns, seq_length, 1))\n","  X = X / float(n_vocab)\n","  Y = to_categorical(dataY)\n","  split = int(0.9*len(X))\n","  train_X = X[:split]\n","  eval_X = X[split:]\n","  train_Y = Y[:split]\n","  eval_Y = Y[split:]\n","  return train_X, train_Y, eval_X, eval_Y, char_to_int, int_to_char, n_vocab, dataX, dataY"]},{"cell_type":"markdown","source":["CREATING MODEL:\n","We define a model containing \"units\" LSTM cells stacked up and passed to a dropout layer. Then the out of it softmaxed and obtained as vector with size of vocab_size"],"metadata":{"id":"EDPFTVRTOeLX"}},{"cell_type":"markdown","metadata":{"id":"O2geo7g0knp1"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVxJXE8Xk6sf"},"outputs":[],"source":["def createModel(units, dropout):\n","  model = Sequential()\n","  model.add(LSTM(units, input_shape=(train_X.shape[1], train_X.shape[2])))\n","  model.add(Dropout(dropout))\n","  model.add(Dense(train_Y.shape[1], activation='softmax'))\n","  return model"]},{"cell_type":"markdown","source":["TRAINING MODEL:\n","We use categorical crossentropy as loss function and Adam as optimizer. Then we call the fit function and train the model on the tokenized dataset."],"metadata":{"id":"VLJpZ9sNPShy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDBu_vgZpK7l"},"outputs":[],"source":["def trainingModel(model, optim, epochs, batch_size=128):\n","  model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['acc'])\n","  filepath=\"weights.hdf5\"\n","  checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","  callbacks_list = [checkpoint]\n","  model.fit(train_X, train_Y, epochs=1, batch_size=128, callbacks=callbacks_list)"]},{"cell_type":"markdown","source":["EVALUATION OF MODEL:\n","We just evaluate the model using the eval split we just split in the above functions."],"metadata":{"id":"sYAc0s7zPfCD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLKPv2ycqNue"},"outputs":[],"source":["def evalModel(model, batch_size=128):\n","  results = model.evaluate(eval_X, eval_Y, batch_size=128)\n","  print(\"The test loss and test acc: \",results)"]},{"cell_type":"markdown","source":["GENERATE PREDICTIONS:\n","We predict the next word in the senquence by processing the output vector of the LSTM model."],"metadata":{"id":"FeHWw_H3Po3T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5veSTKIs5De"},"outputs":[],"source":["def predictSentences(model):\n","  start = np.random.randint(0, len(dataX)-1)\n","  pattern = dataX[start]\n","  for i in range(100):\n","    x = np.reshape(pattern, (1, len(pattern), 1))\n","    x = x / float(n_vocab)\n","    prediction = model.predict(x, verbose=0)\n","    index = np.argmax(prediction)\n","    result = int_to_char[index]\n","    seq_in = [int_to_char[value] for value in pattern]\n","    print(result,end=\"\")\n","    pattern.append(index)\n","    pattern = pattern[1:len(pattern)]\n","  print(\"\\nDone.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23016,"status":"ok","timestamp":1709179840830,"user":{"displayName":"Ashiq Firoz","userId":"12908176422254364099"},"user_tz":-330},"id":"FXwFdBFen-NP","outputId":"a7ac0977-aaf4-468e-8702-e13ce6dac13a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Characters:  1115393\n","Total Vocab:  39\n","Total Patterns:  1115293\n"]}],"source":["raw_text = gatheringData(\"input.txt\")\n","train_X, train_Y, eval_X, eval_Y, char_to_int, int_to_char, n_vocab, dataX, dataY = preProcessingData(raw_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgNWhqumoi-C"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zVMKUCcGuevg"},"outputs":[],"source":["# To Do train the model #2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOH7ELxwvRF0"},"outputs":[],"source":["# To Do evaluate the model #3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evNlv_7uyaql"},"outputs":[],"source":["#To Do predict using the model #4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CIZLM2Tx1B0x"},"outputs":[],"source":["#To Do save model #5"]}],"metadata":{"colab":{"provenance":[{"file_id":"1OvdhbB07YWp1qsHihloR2HYjMaXiCTdO","timestamp":1709213099205}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}